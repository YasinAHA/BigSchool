---
theme: default
---

# Lecci√≥n 17: Sentry Alertas y Playbooks

## Respuesta Autom√°tica a Incidents

---

## Agenda

- ¬øPor qu√© Alertas Inteligentes?
- Tipos de Alertas
- Configuraci√≥n de Thresholds
- Playbooks para Respuesta
- Alert Fatigue Prevention
- Integraci√≥n con Teams

---

## El Problema de las Alertas

**Sin alertas inteligentes**:

```
Escenario t√≠pico:
1. Error ocurre en producci√≥n
2. Nadie se da cuenta por horas
3. Usuarios reportan el problema
4. Equipo descubre en p√°nico
5. Fix urgente sin contexto

Resultado: Tiempo de Detecci√≥n = horas
           Tiempo de Respuesta = m√°s horas
```

**Con alertas inteligentes**:

```
1. Error ocurre
2. Alerta inmediata al equipo
3. Contexto completo disponible
4. Fix informado en minutos

Resultado: Tiempo de Detecci√≥n = segundos
           Tiempo de Respuesta = minutos
```

---

## Tipos de Alertas (1/2)

**1. Alertas de Tasa de Error**:

```
Activador: Tasa de error > umbral
Ejemplo: "Error rate 5% (normal: 0.5%)"

Cu√°ndo alertar:
- Tasa de error aumenta 200%+
- Tasa de error > 2% absoluto
- Aparecen nuevos tipos de errores
```

**2. Alertas de Salud de Release**:

```
Activador: Nuevo release degrada m√©tricas
Ejemplo: "v2.1.0 has 3x more errors than v2.0.9"

Cu√°ndo alertar:
- Tasa de crash aumenta
- Nuevos patrones de error
- Impacto de usuario > umbral
```

---

## Tipos de Alertas (2/2)

**3. Alertas de Rendimiento**:

```
Activador: El rendimiento se degrada
Ejemplo: "LCP increased from 1.5s to 4.2s"

Cu√°ndo alertar:
- Core Web Vitals se degradan
- Tiempo de respuesta > presupuesto
- Tasa de timeout aumenta
```

**4. Alertas de Impacto de Usuario**:

```
Activador: Muchos usuarios afectados
Ejemplo: "1,000+ users experiencing errors"

Cu√°ndo alertar:
- Usuarios √∫nicos afectados > umbral
- Segmentos cr√≠ticos de usuarios afectados
- Errores que impactan ingresos
```

---

## Configuraci√≥n de Umbrales

**Umbral de Tasa de Error**:

```typescript {*}{maxHeight:'300px'}
{
  metric: 'error_rate',
  warning: '1%',    // Notificaci√≥n por email
  critical: '2%',   // PagerDuty/Slack inmediato
  comparison: 'previous_period',
  timeWindow: '5 minutes'
}
```

**Umbral de Rendimiento**:

```typescript {*}{maxHeight:'300px'}
{
  metric: 'lcp',
  warning: '3000ms',   // Aproxim√°ndose al presupuesto
  critical: '4000ms',  // Presupuesto excedido
  percentile: 'p95',   // Percentil 95
  comparison: 'baseline'
}
```

---

## Prevenci√≥n de Fatiga de Alertas

**El problema**:

```
‚ùå Demasiadas alertas ‚Üí Equipo las ignora
‚ùå Alertas ruidosas ‚Üí Falsos positivos
‚ùå Sin priorizaci√≥n ‚Üí Todo parece urgente
```

**Soluci√≥n**:

```
‚úÖ Agrupaci√≥n de alertas: Agrupa errores similares
‚úÖ Umbrales inteligentes: Aprende de hist√≥rico
‚úÖ Niveles de prioridad: Critical > Warning > Info
‚úÖ Reglas de silencio: Ignora conocidos temporalmente
```

---

## Agrupaci√≥n de Alertas

**Sin agrupaci√≥n**:

```
Alert 1: TypeError in user-profile (user 123)
Alert 2: TypeError in user-profile (user 456)
Alert 3: TypeError in user-profile (user 789)
...
Alert 100: TypeError in user-profile (user X)
```

‚Üí 100 notificaciones del mismo problema

**Con agrupaci√≥n**:

```
Alert: TypeError in user-profile
Affected: 100 users
Status: CRITICAL
Action: Fix immediately
```

‚Üí 1 notificaci√≥n con contexto completo

---

## Niveles de Prioridad

**üî¥ CR√çTICO** (Respuesta inmediata):

- Errores que impactan ingresos
- Funcionalidad rota visible para usuarios
- P√©rdida/corrupci√≥n de datos
- Incidentes de seguridad

**üü° ADVERTENCIA** (Monitorear, planificar fix):

- Degradaci√≥n de rendimiento o caracter√≠stica no cr√≠tica rota
- Tasa de error aumentada (manejable)

**üîµ INFORMACI√ìN** (Seguimiento, sin urgencia):

- Problemas de servicios de terceros
- Errores esperados (input inv√°lido)
- Bugs de bajo impacto

---

## Playbooks: Respuesta Estructurada

**¬øQu√© es un Playbook?**

```
Un playbook es una gu√≠a paso a paso para responder
a un tipo espec√≠fico de incidente.

Beneficios:
‚úÖ Respuesta r√°pida y consistente
‚úÖ Menos p√°nico en incidentes
‚úÖ Onboarding de nuevo team
‚úÖ Post-mortem m√°s f√°cil
```

---

## Playbook: Tasa de Error Alta

```markdown {*}{maxHeight:'300px'}
## Playbook: Tasa de Error Alta

### Activador

Tasa de error > 2% por 5+ minutos

### Acciones Inmediatas

1. Revisar dashboard de Sentry para detalles del error
2. Identificar versi√≥n de release afectada
3. Verificar si un segmento espec√≠fico de usuarios est√° afectado
4. Verificar estado de infraestructura (AWS, CDN)

### √Årbol de Decisi√≥n

- ¬øNuevo release? ‚Üí Considerar rollback
- ¬øProblema de infraestructura? ‚Üí Escalar/reiniciar
- ¬øProblema de terceros? ‚Üí Habilitar fallback

### Comunicaci√≥n

- Publicar en canal #incidents
- Actualizar p√°gina de estado si afecta usuarios
- Notificar stakeholders si es cr√≠tico

### Resoluci√≥n

- Desplegar fix o hacer rollback
- Monitorear tasa de error por 15 min
- Marcar incidente como resuelto
- Programar post-mortem
```

---

## Playbook: Degradaci√≥n de Rendimiento

```markdown {*}{maxHeight:'300px'}
## Playbook: Degradaci√≥n de Rendimiento

### Activador

LCP > 4s para usuarios p95 por 10+ minutos

### Acciones Inmediatas

1. Revisar dashboard de Performance de Sentry
2. Identificar transacciones lentas
3. Revisar cambios recientes de c√≥digo
4. Verificar latencia de servicios externos

### Investigaci√≥n

- ¬øQueries de base de datos lentas?
- ¬øRespuestas de API demoradas?
- ¬øBundle grande desplegado?
- ¬øProblemas de CDN?

### Mitigaci√≥n

- Habilitar optimizaciones de rendimiento
- Escalar infraestructura
- Cache agresivo
- Rollback si es necesario

### Comunicaci√≥n

- Actualizaci√≥n transparente a usuarios si es severo
- Publicar progreso en canal #performance
```

---

## Integraci√≥n con Equipos

**Integraci√≥n con Slack**:

```typescript {*}{maxHeight:'300px'}
// Notificaci√≥n autom√°tica a Slack
{
  channel: '#alerts',
  message: 'üö® CRITICAL: High error rate',
  details: {
    error_rate: '3.5%',
    affected_users: 1234,
    release: 'v2.1.0'
  },
  actions: [
    'View in Sentry',
    'Start incident',
    'Rollback release'
  ]
}
```

---

## Reglas de Silencio

**Cu√°ndo silenciar**:

```
‚úÖ Problema conocido de terceros (temporal)
‚úÖ Errores esperados durante migraci√≥n
‚úÖ Bugs de baja prioridad programados para fix
‚úÖ Errores en caracter√≠sticas deprecadas
```

**Ejemplo**:

```typescript {*}{maxHeight:'300px'}
{
  rule: 'Silence NetworkError',
  reason: 'Known CDN issue, monitoring',
  duration: '2 hours',
  conditions: {
    error_type: 'NetworkError',
    message_contains: 'cdn.example.com'
  }
}
```

---

## M√©tricas de Alertas

**M√©tricas de Efectividad**:

```
Tiempo de Detecci√≥n (TTD):
< 5 min = Excelente ‚úÖ
5-15 min = Bueno ‚ö†Ô∏è
> 15 min = Necesita mejora ‚ùå

Tiempo de Respuesta (TTR):
< 15 min = Excelente ‚úÖ
15-60 min = Bueno ‚ö†Ô∏è
> 60 min = Necesita mejora ‚ùå

Precisi√≥n de Alertas:
> 90% accionable = Bueno ‚úÖ
70-90% accionable = Necesita ajuste ‚ö†Ô∏è
< 70% accionable = Demasiado ruidoso ‚ùå
```

---

## Proceso de Post-Mortem

**Despu√©s de cada incidente**:

```
1. L√≠nea de tiempo: ¬øQu√© pas√≥ y cu√°ndo?
2. Causa ra√≠z: ¬øPor qu√© sucedi√≥?
3. Impacto: ¬øA qui√©nes afect√≥?
4. Respuesta: ¬øQu√© hicimos bien/mal?
5. Acciones: ¬øC√≥mo prevenimos esto?

NO cultura de culpa
S√ç cultura de aprendizaje
```

---

## Ejercicio 1: Configurar Alert Rule Inteligente

**Prompt**:

```bash {*}{maxHeight:'300px'}
Act√∫a como un SRE configurando alertas inteligentes para prevenir incidents.

CONTEXTO: Alert Fatigue = demasiadas alertas ‚Üí equipo ignora todas. Sin alertas:
problemas descubiertos por usuarios (malo). Alertas inteligentes: threshold
basado en datos reales (error rate > 1% = anormal). Time window: 5 minutos
(evita falsos positivos de spikes moment√°neos). MTTR (Mean Time To Recovery):
objetivo < 15 min (alertas autom√°ticas cr√≠ticas). Sentry Alert Rules: condiciones
+ acciones (email, Slack, PagerDuty). Environment filters: production (NO alertar
en dev). Proactive monitoring: detectar problemas ANTES que usuarios reporten.

TAREA: Configura alert rule en Sentry para detectar error rate alto en cart.

CONFIGURACI√ìN EN SENTRY DASHBOARD:
1. Ir a sentry.io ‚Üí Alerts ‚Üí Create Alert Rule
2. Select project: [tu proyecto]

ALERT RULE CONFIGURATION:
- Name: "High Error Rate - Cart"
- Condition: "When error rate > 1% in 5 minutes"
- Environment filter: production (excluir development)
- Tags filter (opcional): component:cart (si usas tags)

ACTIONS (Notificaciones):
- Send email to: [tu email]
- Send Slack notification to: #alerts (si tienes Slack integrado)
- Opcional: PagerDuty para on-call rotation

TESTING ALERT:
Generar errores artificiales r√°pidamente:
// En browser console o test button
Array.from({length: 15}).forEach(() => {
  Sentry.captureException(new Error('Test alert - high error rate'))
})

VALIDACI√ìN:

1. Errores generados ‚Üí error rate spike > 1%
2. Esperar ~2-3 minutos (Sentry procesa)
3. Verificar:
   - Email recibido: "High Error Rate - Cart (2.3%)"
   - Slack notification (si configurado)
   - Alert tiene link a Sentry issues

VALIDACI√ìN: Alert notification debe llegar con error rate % y link a dashboard

```

**Aprende**: Alertas basadas en thresholds detectan anomal√≠as

antes que usuarios las experimenten

---

## Ejercicio 2: Documentar Playbook de Respuesta

**Prompt**:

```bash {*}{maxHeight:'300px'}

Act√∫a como un incident manager documentando playbook de respuesta para production incidents.

CONTEXTO: Playbook = checklist documentado de pasos para resolver incident
espec√≠fico. Sin playbook: p√°nico ‚Üí investigaci√≥n desde cero ‚Üí tiempo perdido ‚Üí
MTTR alto. Con playbook: pasos claros ‚Üí diagn√≥stico r√°pido ‚Üí fix sistem√°tico ‚Üí
MTTR bajo. Runbook vs Playbook: runbook = procedimiento operacional, playbook =
respuesta a incident. Incident Response: 1) Detect, 2) Diagnose, 3) Fix, 4)
Document. MTTR target: < 15 min para incidents cr√≠ticos. Escalation path:
cu√°ndo escalar a on-call engineer. Documentation as Code: playbook en c√≥digo
(NO wiki externo que nadie lee).

TAREA: Documenta playbook para cart errors como comentario JSDoc en c√≥digo.

UBICACI√ìN:

- Archivo: src/context/CartContext.tsx (o archivo principal de cart)
- Formato: JSDoc comment /** */ al inicio del archivo

PLAYBOOK STRUCTURE:

/**
 * PLAYBOOK: Cart Error Response
 *
 * TRIGGER: Error rate > 2% in cart operations
 *
 * DIAGNOSIS STEPS:
 * 1. Check product API status: /api/health
 * 2. Verify cart data structure in latest Sentry issues
 * 3. Check recent deployments (last 2 hours) in CI/CD
 * 4. Review Sentry breadcrumbs for user journey pattern
 *
 * REMEDIATION:
 * - If API down: Enable fallback mode (static product data)
 * - If data structure issue: Rollback to previous release
 * - If user input issue: Add validation + hotfix deploy
 *
 * INCIDENT MANAGEMENT:
 * - Create incident in Sentry: Mark as "Critical"
 * - Notify team: Post in #incidents Slack channel
 * - Update status page: "Cart experiencing issues"
 *
 * TARGETS:
 * - MTTR: < 15 minutes
 * - Escalation: After 30 min without resolution ‚Üí call on-call
 * - Post-mortem: Required for all > 5 min downtime
 *
 * CONTACT: oncall-engineer@example.com
 */

ELEMENTOS REQUERIDOS:

- TRIGGER: Qu√© condici√≥n activa el playbook
- DIAGNOSIS: Pasos de investigaci√≥n ordenados
- REMEDIATION: Acciones de fix por tipo de problema
- INCIDENT MANAGEMENT: Comunicaci√≥n y tracking
- TARGETS: MTTR y escalation timing

VALIDACI√ìN: Playbook debe estar como comentario en archivo de producci√≥n

```

**Aprende**: Playbooks documentados en c√≥digo reducen MTTR

y p√°nico durante incidents

---

## Puntos Clave

1. **Alertas Inteligentes**: Detecci√≥n en segundos, no horas
2. **Tipos**: Error rate, release health, performance, user impact
3. **Thresholds**: Configurados por severidad
4. **Prevent Fatigue**: Grouping, priorizaci√≥n, silence rules
5. **Playbooks**: Respuesta estructurada y consistente
6. **Integration**: Slack, PagerDuty, equipos
7. **Post-Mortems**: Aprende de cada incident
